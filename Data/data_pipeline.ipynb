{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# pip install groq datasets torch torchvision pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import libraries and set API key\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Add current directory to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from groq_caption_generator import GroqCaptionGenerator\n",
        "from data_loaders import ImageFolderWithMetadata, HuggingFaceDatasetWrapper, prepare_imagefolder_dataset\n",
        "from dataset_processors import FFHQProcessor, EasyPortraitProcessor, LAIONFaceProcessor\n",
        "\n",
        "# Set your Groq API key\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "\n",
        "if not GROQ_API_KEY:\n",
        "    print(\"Warning: GROQ_API_KEY not set!\")\n",
        "else:\n",
        "    print(\"✓ Groq API key loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Dataset Structure\n",
        "\n",
        "Choose your dataset and prepare the ImageFolder structure with metadata.jsonl file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Prepare FFHQ dataset\n",
        "# Uncomment and modify paths as needed\n",
        "\n",
        "# FFHQ_SOURCE = \"./ffhq/images\"\n",
        "# FFHQ_OUTPUT = \"./data/ffhq_processed\"\n",
        "# \n",
        "# processor = FFHQProcessor(FFHQ_SOURCE, FFHQ_OUTPUT)\n",
        "# processor.prepare_structure()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Captions (Distributed Processing)\n",
        "\n",
        "Each team member should process a different batch by setting `START_INDEX` and `END_INDEX`.\n",
        "\n",
        "**Distribution for 70k FFHQ images across 5 team members:**\n",
        "- Member 1: indices 0-14,000\n",
        "- Member 2: indices 14,000-28,000\n",
        "- Member 3: indices 28,000-42,000\n",
        "- Member 4: indices 42,000-56,000\n",
        "- Member 5: indices 56,000-70,000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for your batch\n",
        "DATASET_DIR = \"./data/ffhq_processed/images\"  # Path to images directory\n",
        "OUTPUT_FILE = \"./data/ffhq_processed/metadata.jsonl\"  # Output metadata file\n",
        "START_INDEX = 0  # starting index\n",
        "END_INDEX = None  # ending index (None = process all remaining)\n",
        "\n",
        "# Initialize caption generator\n",
        "generator = GroqCaptionGenerator(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Get all image paths\n",
        "from pathlib import Path\n",
        "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
        "dataset_path = Path(DATASET_DIR)\n",
        "image_paths = sorted([\n",
        "    str(p) for p in dataset_path.iterdir()\n",
        "    if p.suffix.lower() in image_extensions\n",
        "])\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}\")\n",
        "print(f\"Processing indices: {START_INDEX} to {END_INDEX if END_INDEX else len(image_paths)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate captions for your batch\n",
        "import json\n",
        "\n",
        "if END_INDEX is None:\n",
        "    END_INDEX = len(image_paths)\n",
        "\n",
        "batch_paths = image_paths[START_INDEX:END_INDEX]\n",
        "results = []\n",
        "\n",
        "print(f\"Processing {len(batch_paths)} images...\\n\")\n",
        "\n",
        "for idx, image_path in enumerate(batch_paths):\n",
        "    try:\n",
        "        image_name = Path(image_path).name\n",
        "        print(f\"[{idx+1}/{len(batch_paths)}] Processing: {image_name}\")\n",
        "        \n",
        "        caption = generator.generate_caption(image_path)\n",
        "        \n",
        "        result = {\n",
        "            \"file_name\": image_name,\n",
        "            \"text\": caption\n",
        "        }\n",
        "        results.append(result)\n",
        "        \n",
        "        # Save incrementally\n",
        "        with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(result, ensure_ascii=False) + '\\\\n')\n",
        "        \n",
        "        print(f\"  ✓ {caption[:80]}...\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error: {e}\")\n",
        "        print()\n",
        "        continue\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"Completed: {len(results)}/{len(batch_paths)} captions generated\")\n",
        "print(f\"Results saved to: {OUTPUT_FILE}\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display sample captions\n",
        "import json\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Load metadata\n",
        "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    metadata = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "print(f\"Total entries in metadata: {len(metadata)}\\\\n\")\n",
        "\n",
        "# Display first 5 examples\n",
        "for i, entry in enumerate(metadata[:5]):\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"File: {entry['file_name']}\")\n",
        "    print(f\"Caption: {entry['text']}\")\n",
        "    print()\n",
        "    \n",
        "    # Display image if available\n",
        "    image_path = Path(DATASET_DIR) / entry['file_name']\n",
        "    if image_path.exists():\n",
        "        display(Image(str(image_path), width=200))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Data Loader\n",
        "\n",
        "Test that the dataset can be loaded correctly for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ImageFolder dataset loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_root = \"./data/ffhq_processed\"  # Path to your processed dataset\n",
        "\n",
        "# Create dataset\n",
        "dataset = ImageFolderWithMetadata(\n",
        "    root_dir=dataset_root,\n",
        "    metadata_file=\"metadata.jsonl\",\n",
        "    image_size=768,\n",
        "    center_crop=True\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Test loading a sample\n",
        "sample = dataset[0]\n",
        "print(f\"\\\\nSample data:\")\n",
        "print(f\"  Image shape: {sample['pixel_values'].shape}\")\n",
        "print(f\"  Text: {sample['text'][:100]}...\")\n",
        "print(f\"  File name: {sample['file_name']}\")\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "print(f\"\\\\n✓ DataLoader created successfully\")\n",
        "print(f\"  Batch size: 4\")\n",
        "print(f\"  Number of batches: {len(dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Merge Multiple Datasets (Optional)\n",
        "\n",
        "If processing multiple datasets (FFHQ, EasyPortrait, LAION-Face), merge them into a single dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge multiple processed datasets\n",
        "from dataset_processors import merge_datasets\n",
        "\n",
        "# List of processed dataset directories\n",
        "dataset_dirs = [\n",
        "    \"./data/ffhq_processed\",\n",
        "    \"./data/easyportrait_processed\",\n",
        "    \"./data/laion_face_processed\"\n",
        "]\n",
        "\n",
        "# Output directory for merged dataset\n",
        "merged_output = \"./data/merged_dataset\"\n",
        "\n",
        "# Merge datasets\n",
        "merge_datasets(\n",
        "    dataset_dirs=dataset_dirs,\n",
        "    output_dir=merged_output,\n",
        "    metadata_file=\"metadata.jsonl\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
